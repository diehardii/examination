"""
CET4 AIè¾…å¯¼è€å¸ˆé—®ç­”æœåŠ¡
è´Ÿè´£ç®¡ç†å­¦ç”Ÿä¸AIæ•™å¸ˆçš„å¯¹è¯ï¼ŒåŒ…æ‹¬promptç®¡ç†ã€ChromaDBäº¤äº’ã€Coze APIè°ƒç”¨ï¼ˆæ‘˜è¦ï¼‰ã€DeepSeek APIè°ƒç”¨ï¼ˆé—®ç­”ï¼‰
"""
import json
import logging
import requests
from datetime import datetime
from typing import Dict, Any, List, Optional, Generator
import chromadb
from chromadb.config import Settings
import os
import openai
import sys
import random
import threading
from chromadb.errors import NotFoundError
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from common.logger import setup_logger

# ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
logger = setup_logger('cet4_qa_service')


class PromptBlock:
    """å¯¹è¯ä¸Šä¸‹æ–‡å†…å­˜å—"""
    
    def __init__(self):
        self.metadata = {
            "user_id": None,
            "total_rounds": 0
        }
        self.background = {}
        self.summary = {
            "summary_context": "",
            "timestamp": ""
        }
        self.conversation = []
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "metadata": self.metadata,
            "background": self.background,
            "summary": self.summary,
            "conversation": self.conversation
        }
    
    def from_dict(self, data: Dict[str, Any]):
        """ä»å­—å…¸åŠ è½½"""
        self.metadata = data.get("metadata", self.metadata)
        self.background = data.get("background", {})
        self.summary = data.get("summary", self.summary)
        self.conversation = data.get("conversation", [])


class CET4QuestionAnswerService:
    """CET4é—®ç­”æœåŠ¡ç±»"""
    
    # Coze API é…ç½®ï¼ˆä»…ç”¨äºæ‘˜è¦ï¼‰
    COZE_API_BASE = "https://api.coze.cn/v1/workflow/run"
    COZE_API_KEY = os.environ.get('COZE_API_KEY', 'pat_FsFpG2tf6nYuicX2OfAcYQ4cqy9gPtOH3RJeyohv1tt1xgKED53r9BsvsjFFZZJG')
    
    # å·¥ä½œæµIDï¼ˆä»…æ‘˜è¦ä½¿ç”¨Cozeï¼‰
    WORKFLOW_ID_SUMMARY = "7578730107335344147"  # æ‘˜è¦å‹ç¼©çš„å·¥ä½œæµ
    
    # DeepSeek API é…ç½®ï¼ˆç”¨äºé—®ç­”ï¼‰
    DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', 'sk-508f4b7ff14d414fb806d0a2cb0b7b39')
    DEEPSEEK_BASE_URL = "https://api.deepseek.com"
    DEEPSEEK_MODEL = "deepseek-chat"  # ä½¿ç”¨ deepseek-chat è¿›è¡Œæµå¼å¯¹è¯
    
    # ChromaDBé…ç½®
    CHROMA_PATH = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'chroma')
    COLLECTION_NAME = "tutoring_content_en_cet4"
    EMBEDDING_DIMENSION = 384  # ä¸ Java ç«¯ä¿æŒä¸€è‡´ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹åˆå§‹åŒ– CET4QuestionAnswerService")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– ChromaDBï¼Œè·¯å¾„: {self.CHROMA_PATH}")
            self.chroma_client = chromadb.PersistentClient(
                path=self.CHROMA_PATH,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            # è·å–æˆ–åˆ›å»ºé›†åˆï¼›è‹¥é‡åˆ° HNSW/ç´¢å¼•æŸåé”™è¯¯ï¼Œå°è¯•åˆ é™¤åé‡å»º
            self.collection = self._get_or_recreate_collection()
            logger.info(f"âœ… ChromaDB initialized: {self.CHROMA_PATH}")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize ChromaDB: {e}")
            logger.exception("ChromaDB åˆå§‹åŒ–å¤±è´¥è¯¦æƒ…:")
            raise
        
        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯")
            logger.info(f"  Base URL: {self.DEEPSEEK_BASE_URL}")
            logger.info(f"  Model: {self.DEEPSEEK_MODEL}")
            self.deepseek_client = openai.OpenAI(
                api_key=self.DEEPSEEK_API_KEY,
                base_url=self.DEEPSEEK_BASE_URL
            )
            logger.info(f"âœ… DeepSeek client initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize DeepSeek client: {e}")
            logger.exception("DeepSeek å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥è¯¦æƒ…:")
            raise
        
        # å†…å­˜ä¸­çš„PromptBlockç¼“å­˜ {user_id_segment_id: PromptBlock}
        self.prompt_blocks: Dict[str, PromptBlock] = {}
        
        logger.info("=" * 60)
        logger.info("âœ… CET4QuestionAnswerService åˆå§‹åŒ–å®Œæˆ")
        logger.info("=" * 60)
    
    def _get_cache_key(self, user_id: int, segment_id: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        æ³¨æ„ï¼šä¸€ä¸ªç”¨æˆ·åªä¿ç•™ä¸€ä¸ªå¯¹è¯å†å²æ‘˜è¦ï¼Œæ‰€ä»¥åªä½¿ç”¨ user_id
        segment_id å‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸ç”¨äºç”Ÿæˆé”®
        """
        return str(user_id)
    
    def initialize_prompt_block(
        self,
        user_id: int,
        segment_id: str,
        question_type: str,
        document: str,
        user_answers: List[Dict[str, Any]]
    ) -> PromptBlock:
        """
        åˆå§‹åŒ–PromptBlock
        
        Args:
            user_id: ç”¨æˆ·ID
            segment_id: é¢˜ç›®ç‰‡æ®µID
            question_type: é¢˜å‹
            document: ChromaDBä¸­çš„document JSONå­—ç¬¦ä¸²
            user_answers: ç”¨æˆ·ç­”æ¡ˆåˆ—è¡¨
        
        Returns:
            åˆå§‹åŒ–çš„PromptBlock
        """
        cache_key = self._get_cache_key(user_id, segment_id)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.prompt_blocks:
            logger.info(f"Found cached PromptBlock for user {user_id}")
            logger.info(f"Current segment: {segment_id}, ç»§ç»­ç´¯ç§¯å¯¹è¯")
            return self.prompt_blocks[cache_key]
        
        logger.info(f"Initializing new PromptBlock for user {user_id}")
        logger.info(f"Current segment: {segment_id}")
        logger.info(f"è¯´æ˜: ç”¨æˆ·çš„æ‰€æœ‰é¢˜ç›®å¯¹è¯å°†ç´¯ç§¯åœ¨åŒä¸€ä¸ªå†å²è®°å½•ä¸­")
        
        prompt_block = PromptBlock()
        
        # 1. åˆå§‹åŒ–metadata
        prompt_block.metadata = {
            "user_id": user_id,
            "total_rounds": 0
        }
        
        # 2. åˆå§‹åŒ–background
        prompt_block.background = {
            "question_type": question_type,
            "subject": "cet4",
            "segment_id": segment_id,
            "Document": document,
            "user_answers": user_answers
        }
        
        # 3. ä»ChromaDBåŠ è½½å†å²æ‘˜è¦å’Œå¯¹è¯
        try:
            # æŸ¥è¯¢å†å²è®°å½•
            results = self.collection.get(
                ids=[cache_key],
                include=["documents", "metadatas"]
            )
            
            if results and results['ids']:
                # æ‰¾åˆ°å†å²è®°å½•
                doc = json.loads(results['documents'][0])
                
                # åŠ è½½æ‘˜è¦
                if 'summary' in doc:
                    prompt_block.summary = doc['summary']
                else:
                    prompt_block.summary = {
                        "summary_context": "",
                        "timestamp": datetime.now().isoformat()
                    }
                
                # åŠ è½½æœ€è¿‘5è½®å¯¹è¯
                if 'conversation' in doc and doc['conversation']:
                    conversations = doc['conversation']
                    # å–æœ€å5è½®
                    last_5_conversations = conversations[-5:] if len(conversations) > 5 else conversations
                    # é‡æ–°ç¼–å·ä¸º1-5
                    prompt_block.conversation = [
                        {
                            **conv,
                            "round_id": idx + 1
                        }
                        for idx, conv in enumerate(last_5_conversations)
                    ]
                    # æ›´æ–°total_rounds
                    prompt_block.metadata["total_rounds"] = len(prompt_block.conversation)
                    logger.info(f"Loaded {len(prompt_block.conversation)} conversation rounds from ChromaDB")
            else:
                # æ²¡æœ‰å†å²è®°å½•ï¼Œä½¿ç”¨ç©ºæ‘˜è¦
                prompt_block.summary = {
                    "summary_context": "",
                    "timestamp": datetime.now().isoformat()
                }
                logger.info("No history found in ChromaDB, starting fresh")
        
        except Exception as e:
            logger.error(f"Error loading from ChromaDB: {e}")
            # å‡ºé”™æ—¶ä½¿ç”¨ç©ºæ‘˜è¦
            prompt_block.summary = {
                "summary_context": "",
                "timestamp": datetime.now().isoformat()
            }
        
        # ç¼“å­˜
        self.prompt_blocks[cache_key] = prompt_block
        
        return prompt_block
    
    def _build_answer_prompt(
        self,
        background: Dict[str, Any],
        summary: Dict[str, Any],
        conversation: List[Dict[str, Any]],
        question: str
    ) -> str:
        """
        æ„å»ºDeepSeeké—®ç­”æç¤ºè¯
        
        Args:
            background: èƒŒæ™¯ä¿¡æ¯
            summary: æ‘˜è¦ä¿¡æ¯
            conversation: å¯¹è¯å†å²
            question: å­¦ç”Ÿé—®é¢˜
        
        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        # æ ¼å¼åŒ–èƒŒæ™¯ä¿¡æ¯
        background_text = f"""ã€å½“å‰é”™é¢˜ä¿¡æ¯ã€‘
é¢˜å‹ï¼š{background.get('question_type', 'N/A')}
ç§‘ç›®ï¼š{background.get('subject', 'N/A')}
é¢˜ç›®IDï¼š{background.get('segment_id', 'N/A')}
é¢˜ç›®å†…å®¹ï¼š{background.get('Document', 'N/A')}
å­¦ç”Ÿç­”æ¡ˆï¼š{json.dumps(background.get('user_answers', []), ensure_ascii=False)}
"""
        
        # æ ¼å¼åŒ–æ‘˜è¦ä¿¡æ¯
        summary_text = ""
        if summary and summary.get('summary_context'):
            summary_text = f"""ã€å­¦ç”Ÿä¸ªäººçŸ¥è¯†æ¡£æ¡ˆã€‘
{summary.get('summary_context', 'æš‚æ— å†å²å­¦ä¹ æ¡£æ¡ˆ')}
"""
        
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        conversation_text = ""
        if conversation:
            conversation_text = "ã€è¿‘æœŸå¯¹è¯ä¸Šä¸‹æ–‡ã€‘\n"
            for conv in conversation:
                conversation_text += f"ç¬¬{conv.get('round_id', '?')}è½®:\n"
                conversation_text += f"å­¦ç”Ÿ: {conv.get('content_of_user', '')}\n"
                conversation_text += f"è€å¸ˆ: {conv.get('content_of_LLM', '')}\n\n"
        
        # ç»„è£…å®Œæ•´æç¤ºè¯
        full_prompt = f"""{background_text}

{conversation_text}

{summary_text}

ã€å­¦ç”Ÿå½“å‰çš„æé—®ã€‘
{question}"""
        
        return full_prompt
    
    def call_deepseek_answer_stream(
        self,
        background: Dict[str, Any],
        summary: Dict[str, Any],
        conversation: List[Dict[str, Any]],
        question: str
    ) -> Generator[str, None, None]:
        """
        è°ƒç”¨DeepSeek APIè¿›è¡Œæµå¼å›ç­”
        
        Args:
            background: èƒŒæ™¯ä¿¡æ¯
            summary: æ‘˜è¦ä¿¡æ¯
            conversation: å¯¹è¯å†å²
            question: å­¦ç”Ÿé—®é¢˜
        
        Yields:
            AIå›ç­”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            logger.info(f"=== DeepSeek Answer Stream è°ƒç”¨å‚æ•° ===")
            logger.info(f"Model: {self.DEEPSEEK_MODEL}")
            logger.info(f"API Base URL: {self.DEEPSEEK_BASE_URL}")
            logger.info(f"Question: {question[:100]}...")
            
            # æ‰“å°PromptBlockè¯¦ç»†å†…å®¹
            logger.info(f"\n=== PromptBlock å†…å®¹ ===")
            logger.info(f"Background: {json.dumps(background, ensure_ascii=False, indent=2)}")
            logger.info(f"Summary: {json.dumps(summary, ensure_ascii=False, indent=2)}")
            logger.info(f"Conversation (å…±{len(conversation)}è½®): {json.dumps(conversation, ensure_ascii=False, indent=2)}")
            
            # æ„å»ºæç¤ºè¯
            user_prompt = self._build_answer_prompt(background, summary, conversation, question)
            
            logger.info(f"\n=== æ„å»ºçš„å®Œæ•´æç¤ºè¯ ===")
            logger.info(user_prompt[:500] + "..." if len(user_prompt) > 500 else user_prompt)
            
            # ç³»ç»Ÿæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€è€å¿ƒç»†è‡´çš„å››çº§/å…­çº§è‹±è¯­è¾…å¯¼è€å¸ˆï¼Œä¸“é—¨å¸®åŠ©å­¦ç”Ÿåˆ†æé”™é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼š

ğŸ¯ åŸåˆ™
1. ç²¾å‡†å®šä½ï¼Œç´§æ‰£ææ–™ï¼šæ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæä¾›çš„ä¸‰ä¸ªä¿¡æ¯æºï¼Œå°¤å…¶æ˜¯å½“å‰é”™é¢˜ã€‚ä¸å¼•å…¥å¤–éƒ¨è¶…çº²çŸ¥è¯†æˆ–å‡è®¾ã€‚
2. è”ç³»å†å²ï¼Œå› äººæ–½æ•™ï¼šåœ¨åˆ†æé”™è¯¯æ—¶ï¼Œè¦ä¸»åŠ¨å…³è”ã€å­¦ç”Ÿä¸ªäººçŸ¥è¯†æ¡£æ¡ˆã€‘ä¸­è®°å½•çš„è¯¥å­¦ç”Ÿçš„é«˜é¢‘å¤±åˆ†ç‚¹æˆ–å·²æŒæ¡ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼š"ä½ ä¹‹å‰åœ¨å¤„ç†é•¿å¯¹è¯è½¬æŠ˜æ—¶æŠŠæ¡å¾—å¾ˆå¥½ï¼Œä½†è¿™æ¬¡åœ¨æ–°é—»å¬åŠ›çš„ä¸»æ—¨é¢˜ä¸Šåˆå‡ºç°äº†ç±»ä¼¼é—®é¢˜â€¦"ã€‚
3. å¼•å¯¼å‘ç°ï¼Œæˆäººä»¥æ¸”ï¼šä¸ç›´æ¥ç»™ç­”æ¡ˆã€‚é€šè¿‡æ‹†è§£é—®é¢˜ã€å›é¡¾çŸ¥è¯†æ¡£æ¡ˆä¸­çš„æŠ€å·§ã€æˆ–å¯¹æ¯”è¿‡å¾€é”™é¢˜ï¼Œå¼•å¯¼å­¦ç”Ÿæ‰¾åˆ°è§£é¢˜æ€è·¯ã€‚å¤šç”¨"ä½ è§‰å¾—â€¦ï¼Ÿ"ã€"æˆ‘ä»¬ä¹‹å‰ç”¨ä»€ä¹ˆæ–¹æ³•å¤„ç†è¿‡ç±»ä¼¼é—®é¢˜ï¼Ÿ"ã€‚
4. ç»“æ„æ¸…æ™°ï¼Œåˆ†æ­¥æ‹†è§£ï¼šæŒ‰ç…§"ç¡®è®¤ä¸ç†è§£ â†’ å…³è”æ¡£æ¡ˆï¼Œè¯Šæ–­æ ¹æº â†’ å¼•å¯¼å¼ç ´é¢˜ â†’ æ€»ç»“ä¸å¼ºåŒ–è®°å¿† â†’ é¼“åŠ±ä¸è¡”æ¥"çš„é€»è¾‘ç»„ç»‡å›ç­”å†…å®¹ã€‚
5. ç§¯æé¼“åŠ±ï¼Œæ­£å‘åé¦ˆï¼šå¯¹å­¦ç”Ÿçš„è¿›æ­¥ï¼ˆå¦‚åœ¨æ¡£æ¡ˆä¸­å·²æ”¹å–„çš„å¼±ç‚¹ï¼‰è¦æ˜ç¡®æŒ‡å‡ºå¹¶è¡¨æ‰¬ï¼Œå»ºç«‹ä¿¡å¿ƒã€‚

ğŸ“ å›ç­”è¦æ±‚ï¼ˆé‡è¦ï¼‰
å›ç­”æ—¶éœ€è¦åŒ…å«ä»¥ä¸‹å†…å®¹ï¼Œä½†ã€ä¸¥ç¦è¾“å‡ºä»»ä½•æ ‡é¢˜ã€ç¼–å·ã€åˆ†éš”ç¬¦ã€‘ï¼Œç›´æ¥ä»¥è‡ªç„¶è¿è´¯çš„è¯­è¨€è¾“å‡ºï¼š
- é¦–å…ˆå¤è¿°é”™é¢˜å…³é”®ä¿¡æ¯ï¼Œç¡®ä¿åŒæ–¹ç†è§£ä¸€è‡´
- ç„¶åç»“åˆå­¦ç”Ÿæ¡£æ¡ˆåˆ†æé”™è¯¯åŸå› 
- æ¥ç€é€šè¿‡æé—®å¼•å¯¼å­¦ç”Ÿæ‰¾åˆ°è§£é¢˜æ–¹æ³•
- éšåæç‚¼æ ¸å¿ƒæ•™è®­å’Œæ–¹æ³•
- æœ€åç»™äºˆæ­£å‘åé¦ˆå’Œé¼“åŠ±

âŒ ç¦æ­¢è¾“å‡ºï¼š
- ä»»ä½•åŠ ç²—æ ‡é¢˜ï¼ˆå¦‚ **ç¡®è®¤ä¸ç†è§£**ã€**è¯Šæ–­æ ¹æº** ç­‰ï¼‰
- ä»»ä½•ç¼–å·ï¼ˆå¦‚ 1.ã€2.ã€3. æˆ– ä¸€ã€äºŒã€ä¸‰ï¼‰
- ä»»ä½•åˆ†éš”ç¬¦ï¼ˆå¦‚ ---ã€===ã€###ï¼‰

âœ… æ­£ç¡®ç¤ºä¾‹ï¼š
"æˆ‘çœ‹åˆ°ä½ åœ¨è¿™é“é•¿å¯¹è¯é¢˜ä¸­é€‰æ‹©äº†Bé€‰é¡¹ã€‚è®©æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™é“é¢˜..."

è¯·ç”¨è‡ªç„¶æµç•…çš„å¯¹è¯æ–¹å¼è¾“å‡ºï¼Œè®©å›ç­”åƒä¸€ä½è€å¸ˆåœ¨é¢å¯¹é¢äº¤æµï¼Œè€Œä¸æ˜¯åœ¨å¡«å†™ç»“æ„åŒ–çš„è¡¨æ ¼ã€‚"""
            
            logger.info(f"\n=== å¼€å§‹è°ƒç”¨DeepSeek Stream API ===")
            
            # è°ƒç”¨DeepSeekæµå¼APIï¼ˆä½¿ç”¨ openai åº“ï¼‰
            response = self.deepseek_client.chat.completions.create(
                model=self.DEEPSEEK_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                stream=True,
                temperature=0.7,
                max_tokens=2000
            )
            
            logger.info("DeepSeek Stream API è°ƒç”¨æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æµå¼å“åº”")
            
            # æµå¼è¿”å›å†…å®¹
            chunk_count = 0
            total_content_length = 0
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    chunk_count += 1
                    total_content_length += len(content)
                    
                    # æ‰“å°ç¬¬ä¸€ä¸ªå’Œæ¯100ä¸ªchunkçš„ä¿¡æ¯
                    if chunk_count == 1 or chunk_count % 100 == 0:
                        logger.info(f"[DeepSeek] æ”¶åˆ°ç¬¬ {chunk_count} ä¸ªchunkï¼Œå†…å®¹é•¿åº¦: {len(content)}, ç´¯è®¡: {total_content_length}")
                    
                    yield content
            
            logger.info(f"DeepSeek Stream API å“åº”å®Œæˆï¼Œå…± {chunk_count} ä¸ªchunksï¼Œæ€»é•¿åº¦ {total_content_length} å­—ç¬¦")
        
        except Exception as e:
            logger.error(f"Failed to call DeepSeek stream API: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯å †æ ˆ:")
            raise
    
    def call_coze_summary_workflow(
        self,
        summary_before: Dict[str, Any],
        conversations: List[Dict[str, Any]]
    ) -> str:
        """
        è°ƒç”¨Cozeæ‘˜è¦å·¥ä½œæµ
        
        Args:
            summary_before: ä¹‹å‰çš„æ‘˜è¦
            conversations: éœ€è¦å‹ç¼©çš„å¯¹è¯
        
        Returns:
            æ–°çš„æ‘˜è¦å†…å®¹
        """
        try:
            logger.info(f"\n=== Coze Summary Workflow è°ƒç”¨å‚æ•° ===")
            logger.info(f"Workflow ID: {self.WORKFLOW_ID_SUMMARY}")
            logger.info(f"API Base URL: {self.COZE_API_BASE}")
            logger.info(f"API Token: {self.COZE_API_KEY[:20]}...{self.COZE_API_KEY[-10:]}")
            
            # æ‰“å°æ‘˜è¦å‰çš„å†…å®¹
            logger.info(f"\n=== Summary è¾“å…¥å†…å®¹ ===")
            logger.info(f"Summary Before: {json.dumps(summary_before, ensure_ascii=False, indent=2)}")
            logger.info(f"Conversations to compress (å…±{len(conversations)}è½®): {json.dumps(conversations, ensure_ascii=False, indent=2)}")
            
            headers = {
                "Authorization": f"Bearer {self.COZE_API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "workflow_id": self.WORKFLOW_ID_SUMMARY,
                "parameters": {
                    "summary_before": json.dumps(summary_before, ensure_ascii=False),
                    "conversations": json.dumps(conversations, ensure_ascii=False)
                }
            }
            
            logger.info(f"\n=== å¼€å§‹è°ƒç”¨Coze Summary API ===")
            
            response = requests.post(
                self.COZE_API_BASE,
                headers=headers,
                json=payload,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            logger.info(f"Summary Response Status: {response.status_code}")
            logger.info(f"Summary Result code: {result.get('code')}")
            
            # æå–è¾“å‡º
            if result.get('code') == 0 and result.get('data'):
                data = result['data']
                
                # dataå¯èƒ½æ˜¯å­—ç¬¦ä¸²(éœ€è¦è§£æ)æˆ–å·²ç»æ˜¯dict
                if isinstance(data, str):
                    logger.info("Summary data is string, parsing JSON...")
                    data = json.loads(data)
                
                output = data.get('output', '')
                logger.info(f"\n=== Coze Summary Workflow å“åº”æˆåŠŸ ===")
                logger.info(f"Summary Output length: {len(output)} å­—ç¬¦")
                logger.info(f"Summary Output content: {output}")  # æ‰“å°å®Œæ•´æ‘˜è¦å†…å®¹
                logger.info(f"Summary Usage: {result.get('usage', {})}")
                logger.info(f"Summary Execute ID: {result.get('execute_id', 'N/A')}")
                return output
            else:
                error_msg = result.get('msg', 'Unknown error')
                logger.error(f"Coze API error: {error_msg}")
                raise Exception(f"Coze API error: {error_msg}")
        
        except Exception as e:
            logger.error(f"Failed to call Coze summary workflow: {e}")
            raise
    
    def process_question(
        self,
        user_id: int,
        segment_id: str,
        question_type: str,
        document: str,
        user_answers: List[Dict[str, Any]],
        question: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†å­¦ç”Ÿæé—®ï¼ˆéæµå¼ï¼Œå®Œæ•´è¿”å›ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            segment_id: é¢˜ç›®ç‰‡æ®µID
            question_type: é¢˜å‹
            document: é¢˜ç›®document
            user_answers: ç”¨æˆ·ç­”æ¡ˆ
            question: å­¦ç”Ÿé—®é¢˜
        
        Returns:
            åŒ…å«AIå›ç­”çš„å“åº”
        """
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"=== process_question å¼€å§‹ ===")
            logger.info(f"{'='*60}")
            logger.info(f"User ID: {user_id}")
            logger.info(f"Segment ID: {segment_id}")
            logger.info(f"Question Type: {question_type}")
            logger.info(f"Question: {question}")
            
            # 1. åˆå§‹åŒ–æˆ–è·å–PromptBlock
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤1: åˆå§‹åŒ–PromptBlock")
            logger.info(f"{'='*60}")
            prompt_block = self.initialize_prompt_block(
                user_id, segment_id, question_type, document, user_answers
            )
            logger.info(f"PromptBlockåˆå§‹åŒ–å®Œæˆ")
            
            # 2. è°ƒç”¨DeepSeekè·å–å®Œæ•´å›ç­”
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤2: è°ƒç”¨DeepSeek APIè·å–å›ç­”")
            logger.info(f"{'='*60}")
            
            # æ”¶é›†å®Œæ•´ç­”æ¡ˆ
            full_answer = ""
            for chunk in self.call_deepseek_answer_stream(
                background=prompt_block.background,
                summary=prompt_block.summary,
                conversation=prompt_block.conversation,
                question=question
            ):
                full_answer += chunk
            
            logger.info(f"DeepSeek å®Œæ•´å›ç­”é•¿åº¦: {len(full_answer)} å­—ç¬¦")
            
            # 3. æ›´æ–°å¯¹è¯å†å²
            prompt_block.metadata["total_rounds"] += 1
            round_id = prompt_block.metadata["total_rounds"]
            
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤3: æ›´æ–°å¯¹è¯å†å²")
            logger.info(f"{'='*60}")
            
            new_round = {
                "round_id": round_id,
                "content_of_user": question,
                "content_of_LLM": full_answer,
                "timestamp": datetime.now().isoformat()
            }
            
            prompt_block.conversation.append(new_round)
            logger.info(f"æ–°å¢ç¬¬ {round_id} è½®å¯¹è¯")
            
            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©ï¼ˆè¶…è¿‡10è½®ï¼‰
            logger.info(f"\n{'='*60}")
            logger.info(f"æ­¥éª¤4: æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼© (å½“å‰è½®æ¬¡: {prompt_block.metadata['total_rounds']})")
            logger.info(f"{'='*60}")
            
            if prompt_block.metadata["total_rounds"] >= 10:
                logger.info(f"âš ï¸ å¯¹è¯è¾¾åˆ°/è¶…è¿‡10è½®ï¼Œå¼€å§‹å‹ç¼©å‰5è½®...")
                self._run_async(self._compress_conversation, prompt_block)
                logger.info(f"âœ… å·²å¼‚æ­¥è§¦å‘å‹ç¼©ï¼Œå½“å‰å¯¹è¯è½®æ•°ï¼ˆå‹ç¼©åå°†æ›´æ–°ï¼‰: {len(prompt_block.conversation)}")
            else:
                logger.info(f"å¯¹è¯æœªè¶…è¿‡10è½®ï¼Œæ— éœ€å‹ç¼©")
            
            # 5. è¿”å›ç»“æœ
            logger.info(f"\n{'='*60}")
            logger.info(f"=== process_question å®Œæˆ ===")
            logger.info(f"{'='*60}")
            logger.info(f"âœ… æˆåŠŸè¿”å›ç­”æ¡ˆï¼Œè½®æ¬¡: {round_id}/{prompt_block.metadata['total_rounds']}")
            
            return {
                "success": True,
                "answer": full_answer,
                "round_id": round_id,
                "total_rounds": prompt_block.metadata["total_rounds"]
            }
        
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯å †æ ˆ:")
            return {
                "success": False,
                "error": str(e)
            }

    def process_question_stream(
        self,
        user_id: int,
        segment_id: str,
        question_type: str,
        document: str,
        user_answers: List[Dict[str, Any]],
        question: str
    ) -> Generator[Dict[str, Any], None, None]:
        """
        å¤„ç†å­¦ç”Ÿæé—®ï¼ˆæµå¼ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            segment_id: é¢˜ç›®ç‰‡æ®µID
            question_type: é¢˜å‹
            document: é¢˜ç›®document
            user_answers: ç”¨æˆ·ç­”æ¡ˆ
            question: å­¦ç”Ÿé—®é¢˜
        
        Yields:
            åŒ…å«æµå¼å“åº”çš„å­—å…¸
        """
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"=== process_question_stream å¼€å§‹ ===")
            logger.info(f"{'='*60}")
            logger.info(f"User ID: {user_id}")
            logger.info(f"Segment ID: {segment_id}")
            logger.info(f"Question Type: {question_type}")
            logger.info(f"Question: {question}")
            
            # 1. åˆå§‹åŒ–æˆ–è·å–PromptBlock
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤1: åˆå§‹åŒ–PromptBlock")
            logger.info(f"{'='*60}")
            prompt_block = self.initialize_prompt_block(
                user_id, segment_id, question_type, document, user_answers
            )
            logger.info(f"PromptBlockåˆå§‹åŒ–å®Œæˆ")
            logger.info(f"\n=== å®Œæ•´ PromptBlock å†…å®¹ ===")
            logger.info(f"Metadata: {json.dumps(prompt_block.metadata, ensure_ascii=False, indent=2)}")
            logger.info(f"Background: {json.dumps(prompt_block.background, ensure_ascii=False, indent=2)}")
            logger.info(f"Summary: {json.dumps(prompt_block.summary, ensure_ascii=False, indent=2)}")
            logger.info(f"Conversation History (å…±{len(prompt_block.conversation)}è½®):")
            for idx, conv in enumerate(prompt_block.conversation, 1):
                logger.info(f"  è½®æ¬¡{idx}: {json.dumps(conv, ensure_ascii=False, indent=4)}")
            
            # 2. è°ƒç”¨DeepSeekè·å–æµå¼å›ç­”
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤2: è°ƒç”¨DeepSeek APIè·å–æµå¼å›ç­”")
            logger.info(f"{'='*60}")
            
            # å‘é€å¼€å§‹äº‹ä»¶
            yield {"type": "start", "message": "æ­£åœ¨æ€è€ƒ..."}
            
            # æ”¶é›†å®Œæ•´ç­”æ¡ˆç”¨äºä¿å­˜
            full_answer = ""
            
            # æµå¼è¿”å›ç­”æ¡ˆ
            chunk_count = 0
            for chunk in self.call_deepseek_answer_stream(
                background=prompt_block.background,
                summary=prompt_block.summary,
                conversation=prompt_block.conversation,
                question=question
            ):
                chunk_count += 1
                full_answer += chunk
                if chunk_count % 10 == 1:  # æ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡æ—¥å¿—
                    logger.info(f"æ”¶åˆ°ç¬¬ {chunk_count} ä¸ªchunkï¼Œå½“å‰æ€»é•¿åº¦: {len(full_answer)}")
                yield {"type": "chunk", "content": chunk}
            
            logger.info(f"å…±æ”¶åˆ° {chunk_count} ä¸ªchunks")
            
            logger.info(f"DeepSeek å®Œæ•´å›ç­”é•¿åº¦: {len(full_answer)} å­—ç¬¦")
            
            # 3. æ›´æ–°å¯¹è¯å†å²
            prompt_block.metadata["total_rounds"] += 1
            round_id = prompt_block.metadata["total_rounds"]
            
            logger.info(f"\n{'='*60}")
            logger.info("æ­¥éª¤3: æ›´æ–°å¯¹è¯å†å²")
            logger.info(f"{'='*60}")
            
            new_round = {
                "round_id": round_id,
                "content_of_user": question,
                "content_of_LLM": full_answer,
                "timestamp": datetime.now().isoformat()
            }
            
            prompt_block.conversation.append(new_round)
            logger.info(f"æ–°å¢ç¬¬ {round_id} è½®å¯¹è¯")
            logger.info(f"æ–°å¯¹è¯å†…å®¹: {json.dumps(new_round, ensure_ascii=False, indent=2)}")
            
            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©ï¼ˆè¶…è¿‡10è½®ï¼‰
            logger.info(f"\n{'='*60}")
            logger.info(f"æ­¥éª¤4: æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼© (å½“å‰è½®æ¬¡: {prompt_block.metadata['total_rounds']})")
            logger.info(f"{'='*60}")
            
            if prompt_block.metadata["total_rounds"] >= 10:
                logger.info(f"âš ï¸ å¯¹è¯è¾¾åˆ°/è¶…è¿‡10è½®ï¼Œå¼€å§‹å‹ç¼©å‰5è½®...")
                self._run_async(self._compress_conversation, prompt_block)
                logger.info(f"âœ… å·²å¼‚æ­¥è§¦å‘å‹ç¼©ï¼Œå½“å‰å¯¹è¯è½®æ•°ï¼ˆå‹ç¼©åå°†æ›´æ–°ï¼‰: {len(prompt_block.conversation)}")
            else:
                logger.info(f"å¯¹è¯æœªè¶…è¿‡10è½®ï¼Œæ— éœ€å‹ç¼©")
            
            # 5. å‘é€å®Œæˆäº‹ä»¶
            logger.info(f"\n{'='*60}")
            logger.info(f"=== process_question_stream å®Œæˆ ===")
            logger.info(f"{'='*60}")
            logger.info(f"âœ… æˆåŠŸè¿”å›ç­”æ¡ˆï¼Œè½®æ¬¡: {round_id}/{prompt_block.metadata['total_rounds']}")
            
            yield {
                "type": "done",
                "round_id": round_id,
                "total_rounds": prompt_block.metadata["total_rounds"]
            }
        
        except Exception as e:
            logger.error(f"Error processing question stream: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯å †æ ˆ:")
            yield {"type": "error", "message": str(e)}
    
    def _compress_conversation(self, prompt_block: PromptBlock):
        """å‹ç¼©å¯¹è¯å†å²ï¼ˆå½“è¶…è¿‡10è½®æ—¶ï¼‰"""
        try:
            logger.info(f"\n{'='*60}")
            logger.info("=== å¼€å§‹å‹ç¼©å¯¹è¯ ===")
            logger.info(f"{'='*60}")
            
            # æ‰“å°è°ƒç”¨æ‘˜è¦å‰çš„å®Œæ•´ prompt_block å†…å®¹
            logger.info(f"\n=== è°ƒç”¨æ‘˜è¦å‰çš„ PromptBlock å®Œæ•´å†…å®¹ ===")
            logger.info(f"Metadata: {json.dumps(prompt_block.metadata, ensure_ascii=False, indent=2)}")
            logger.info(f"Background: {json.dumps(prompt_block.background, ensure_ascii=False, indent=2)}")
            logger.info(f"å½“å‰Summary: {json.dumps(prompt_block.summary, ensure_ascii=False, indent=2)}")
            logger.info(f"å½“å‰Conversation (å…±{len(prompt_block.conversation)}è½®): {json.dumps(prompt_block.conversation, ensure_ascii=False, indent=2)}")
            
            # å–å‡ºå‰5è½®å¯¹è¯
            conversations_to_compress = prompt_block.conversation[:5]
            logger.info(f"\nå¾…å‹ç¼©çš„å¯¹è¯ (å‰5è½®): {json.dumps(conversations_to_compress, ensure_ascii=False, indent=2)}")
            
            # è°ƒç”¨æ‘˜è¦å·¥ä½œæµ
            logger.info(f"\nğŸ”„ å¼€å§‹è°ƒç”¨æ‘˜è¦å·¥ä½œæµ...")
            new_summary_context = self.call_coze_summary_workflow(
                summary_before=prompt_block.summary,
                conversations=conversations_to_compress
            )
            
            # æ›´æ–°æ‘˜è¦
            old_summary = prompt_block.summary.copy()
            prompt_block.summary = {
                "summary_context": new_summary_context,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"\n=== âœ… æ‘˜è¦å‹ç¼©å®Œæˆ ===")
            logger.info(f"ğŸ“‹ æ—§æ‘˜è¦: {json.dumps(old_summary, ensure_ascii=False, indent=2)}")
            logger.info(f"âœ¨ æ–°æ‘˜è¦: {json.dumps(prompt_block.summary, ensure_ascii=False, indent=2)}")
            
            # åˆ é™¤å‰5è½®ï¼Œä¿ç•™å5è½®
            remaining_conversations = prompt_block.conversation[5:]
            logger.info(f"\nä¿ç•™çš„å¯¹è¯ (å{len(remaining_conversations)}è½®)")
            
            # é‡æ–°ç¼–å·ä¸º1-5
            prompt_block.conversation = [
                {
                    **conv,
                    "round_id": idx + 1
                }
                for idx, conv in enumerate(remaining_conversations)
            ]
            
            # é‡ç½®æ€»è½®æ•°
            prompt_block.metadata["total_rounds"] = len(prompt_block.conversation)
            
            logger.info(f"\nâœ… å¯¹è¯å‹ç¼©å®Œæˆ")
            logger.info(f"å‹ç¼©åå¯¹è¯è½®æ•°: {len(prompt_block.conversation)}")
            logger.info(f"å‹ç¼©åçš„å¯¹è¯: {json.dumps(prompt_block.conversation, ensure_ascii=False, indent=2)}")
        
        except Exception as e:
            logger.error(f"Error compressing conversation: {e}")
            raise
    
    def save_to_chromadb(self, user_id: int, segment_id: str):
        """
        ä¿å­˜PromptBlockåˆ°ChromaDB
        
        Args:
            user_id: ç”¨æˆ·ID
            segment_id: ç‰‡æ®µID
        """
        try:
            cache_key = self._get_cache_key(user_id, segment_id)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"=== å¼€å§‹ä¿å­˜ä¼šè¯åˆ°ChromaDB ===")
            logger.info(f"{'='*60}")
            logger.info(f"User ID: {user_id}")
            logger.info(f"Segment ID: {segment_id} (ä»…ç”¨äºè®°å½•ï¼Œä¸å½±å“å­˜å‚¨)")
            logger.info(f"Storage Key (ä»…ç”¨æˆ·ID): {cache_key}")
            logger.info(f"è¯´æ˜: ä¸€ä¸ªç”¨æˆ·åªä¿ç•™ä¸€ä¸ªå¯¹è¯å†å²æ‘˜è¦")
            logger.info(f"å½“å‰ç¼“å­˜çš„æ‰€æœ‰ä¼šè¯: {list(self.prompt_blocks.keys())}")
            
            if cache_key not in self.prompt_blocks:
                logger.warning(f"âš ï¸ ç¼“å­˜ä¸­æœªæ‰¾åˆ° PromptBlock: {cache_key}")
                logger.warning(f"å¯èƒ½åŸå› ï¼š1) æ²¡æœ‰è¿›è¡Œè¿‡å¯¹è¯ 2) ä¼šè¯å·²ç»è¢«ä¿å­˜è¿‡äº†")
                return
            
            prompt_block = self.prompt_blocks[cache_key]
            logger.info(f"âœ… æ‰¾åˆ° PromptBlockï¼Œå¯¹è¯è½®æ•°: {len(prompt_block.conversation)}")
            
            # å¦‚æœæœ‰æœªå‹ç¼©çš„å¯¹è¯ï¼Œå…ˆåšæœ€ç»ˆæ‘˜è¦
            if len(prompt_block.conversation) > 0:
                logger.info(f"\n{'='*60}")
                logger.info("=== ä¿å­˜å‰æ‰§è¡Œæœ€ç»ˆæ‘˜è¦ ===")
                logger.info(f"{'='*60}")
                
                # æ‰“å°è°ƒç”¨æ‘˜è¦å‰çš„å®Œæ•´ prompt_block å†…å®¹
                logger.info(f"\n=== è°ƒç”¨æœ€ç»ˆæ‘˜è¦å‰çš„ PromptBlock å®Œæ•´å†…å®¹ ===")
                logger.info(f"Metadata: {json.dumps(prompt_block.metadata, ensure_ascii=False, indent=2)}")
                logger.info(f"Background: {json.dumps(prompt_block.background, ensure_ascii=False, indent=2)}")
                logger.info(f"å½“å‰Summary: {json.dumps(prompt_block.summary, ensure_ascii=False, indent=2)}")
                logger.info(f"å½“å‰Conversation (å…±{len(prompt_block.conversation)}è½®): {json.dumps(prompt_block.conversation, ensure_ascii=False, indent=2)}")
                
                logger.info(f"\nğŸ”„ å¼€å§‹è°ƒç”¨æœ€ç»ˆæ‘˜è¦å·¥ä½œæµ...")
                new_summary_context = self.call_coze_summary_workflow(
                    summary_before=prompt_block.summary,
                    conversations=prompt_block.conversation
                )
                
                old_summary = prompt_block.summary.copy()
                prompt_block.summary = {
                    "summary_context": new_summary_context,
                    "timestamp": datetime.now().isoformat()
                }
                
                logger.info(f"\n=== âœ… æœ€ç»ˆæ‘˜è¦å®Œæˆ ===")
                logger.info(f"ğŸ“‹ æ—§æ‘˜è¦: {json.dumps(old_summary, ensure_ascii=False, indent=2)}")
                logger.info(f"âœ¨ æ–°æ‘˜è¦: {json.dumps(prompt_block.summary, ensure_ascii=False, indent=2)}")
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            document_data = {
                "summary": prompt_block.summary,
                "conversation": prompt_block.conversation
            }
            
            metadata = {
                "user_id": str(user_id),
                "timestamp": datetime.now().isoformat(),
                "subject": "cet4",
                "last_segment_id": segment_id  # è®°å½•æœ€åä¸€æ¬¡å¯¹è¯çš„ç‰‡æ®µID
            }
            
            # å­˜å‚¨åˆ°ChromaDB
            # æ³¨æ„ï¼šChromaDB éœ€è¦ embeddingsï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªå ä½å‘é‡
            # å› ä¸ºæˆ‘ä»¬åªæ˜¯å­˜å‚¨å¯¹è¯å†å²ï¼Œä¸éœ€è¦å‘é‡æœç´¢åŠŸèƒ½
            # ID åªä½¿ç”¨ user_idï¼Œä¸€ä¸ªç”¨æˆ·åªä¿ç•™ä¸€ä¸ªå¯¹è¯å†å²æ‘˜è¦
            collection = self._ensure_collection()
            collection.upsert(
                ids=[cache_key],  # cache_key = str(user_id)
                documents=[json.dumps(document_data, ensure_ascii=False)],
                metadatas=[metadata],
                embeddings=[self._generate_deterministic_embedding(cache_key)]
            )
            
            logger.info(f"âœ… Saved PromptBlock to ChromaDB")
            logger.info(f"   Storage ID (ç”¨æˆ·ID): {cache_key}")
            logger.info(f"   Last Segment ID: {segment_id}")
            logger.info(f"   Document size: {len(json.dumps(document_data, ensure_ascii=False))} bytes")
            logger.info(f"   Conversation rounds: {len(prompt_block.conversation)}")
            logger.info(f"   è¯´æ˜: è¯¥ç”¨æˆ·çš„æ‰€æœ‰é¢˜ç›®å¯¹è¯éƒ½ç´¯ç§¯åœ¨æ­¤è®°å½•ä¸­")
            
            # æ¸…é™¤ç¼“å­˜
            del self.prompt_blocks[cache_key]
        
        except Exception as e:
            logger.error(f"Error saving to ChromaDB: {e}")
            raise
    
    def end_session(self, user_id: int, segment_id: str):
        """
        ç»“æŸå¯¹è¯ä¼šè¯ï¼Œä¿å­˜åˆ°ChromaDB
        
        Args:
            user_id: ç”¨æˆ·ID
            segment_id: ç‰‡æ®µID
        """
        self.save_to_chromadb_async(user_id, segment_id)

    def save_to_chromadb_async(self, user_id: int, segment_id: str):
        """å¼‚æ­¥ä¿å­˜ï¼Œå‰ç«¯æ— éœ€ç­‰å¾…"""
        self._run_async(self.save_to_chromadb, user_id, segment_id)

    def _generate_deterministic_embedding(self, seed: str) -> List[float]:
        """ç”Ÿæˆä¸ Java ç«¯ä¸€è‡´çš„å›ºå®šé•¿åº¦å ä½ embeddingï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…"""
        rng = random.Random(seed or 0)
        return [rng.random() for _ in range(self.EMBEDDING_DIMENSION)]

    def _run_async(self, target, *args, **kwargs):
        """å¯åŠ¨åå°çº¿ç¨‹è¿è¡Œé•¿è€—æ—¶ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ã€ä¿å­˜ï¼‰"""
        def _wrapper():
            try:
                target(*args, **kwargs)
            except Exception as e:
                logger.error(f"åå°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        threading.Thread(target=_wrapper, daemon=True).start()

    def _get_or_recreate_collection(self):
        """å°½é‡å¤åŸæŸåçš„é›†åˆï¼ˆHNSW/ç´¢å¼•ç¼ºå¤±æ—¶åˆ é™¤é‡å»ºï¼‰"""
        metadata = {
            "description": "CET4 tutoring conversation history",
            "hnsw:space": "l2"
        }
        try:
            return self.chroma_client.get_or_create_collection(
                name=self.COLLECTION_NAME,
                metadata=metadata
            )
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–é›†åˆå¤±è´¥ï¼Œå°è¯•åˆ é™¤é‡å»º: {e}")
            try:
                self.chroma_client.delete_collection(name=self.COLLECTION_NAME)
            except Exception as del_err:
                logger.warning(f"åˆ é™¤æŸåé›†åˆå¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {del_err}")
            return self.chroma_client.get_or_create_collection(
                name=self.COLLECTION_NAME,
                metadata=metadata
            )

    def _ensure_collection(self):
        """ç¡®ä¿ collection å¥æŸ„æœ‰æ•ˆï¼›è‹¥è¢«å¤–éƒ¨åˆ é™¤åˆ™é‡å»ºå¹¶æ›´æ–° self.collection"""
        try:
            # è§¦å‘ä¸€æ¬¡è½»é‡æ“ä½œï¼Œè‹¥å¥æŸ„å¤±æ•ˆä¼šæŠ› NotFound
            self.collection.count()
            return self.collection
        except NotFoundError:
            logger.warning("æ£€æµ‹åˆ°é›†åˆå¥æŸ„å¤±æ•ˆï¼Œå°è¯•é‡æ–°è·å–/é‡å»º")
        except Exception as e:
            logger.warning(f"é›†åˆå¥æŸ„æ£€æŸ¥å¼‚å¸¸ï¼Œå°†å°è¯•é‡å»º: {e}")
        self.collection = self._get_or_recreate_collection()
        return self.collection


# åˆ›å»ºæœåŠ¡å®ä¾‹
logger.info("ğŸš€ æ­£åœ¨åˆ›å»º CET4QuestionAnswerService å®ä¾‹...")
qa_service = CET4QuestionAnswerService()
logger.info("ğŸ‰ qa_service å®ä¾‹åˆ›å»ºæˆåŠŸï¼")

